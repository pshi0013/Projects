{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Streaming application using Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write code to create a SparkSession with the following requirements: 1) use four cores with a proper application name; 2) Melbourne timezone; 3) a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Set Spark application name and master\n",
    "app_name = \"Assignment 2B\"\n",
    "master = \"local[4]\"\n",
    "\n",
    "# Create Spark configuration\n",
    "spark_conf = SparkConf().setAppName(app_name).setMaster(master)\\\n",
    "                        .set(\"spark.sql.session.timeZone\", \"Australia/Melbourne\")\\\n",
    "                        .set(\"spark.driver.memory\", \"8g\")\\\n",
    "                        .set(\"spark.checkpoint.auto\", \"true\")\\\n",
    "                        .set(\"checkpoint_location\", \"checkpoint\")\\\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.config(conf = spark_conf)\\\n",
    "                            .config(\"spark.sql.streaming.statefulOperator.allowMultiple\", \"false\") \\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Similar to assignment 2A, write code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets(previous_application_static and value_dict) into data frames. (You can reuse your code from 2A.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.sql.functions import explode\n",
    "\n",
    "pre_app_schema = StructType([\n",
    "    StructField(\"id_app\", IntegerType()),\n",
    "    StructField(\"contract_type\", IntegerType()),\n",
    "    StructField(\"amt_annuity\", FloatType()),\n",
    "    StructField(\"amt_application\", FloatType()),\n",
    "    StructField(\"amt_credit\", FloatType()),\n",
    "    StructField(\"amt_down_payment\", FloatType()),\n",
    "    StructField(\"amt_goods_price\", FloatType()),\n",
    "    StructField(\"hour_appr_process_start\", IntegerType()),\n",
    "    StructField(\"rate_down_payment\", FloatType()),\n",
    "    StructField(\"rate_interest_primary\", FloatType()),\n",
    "    StructField(\"rate_interest_privileged\", FloatType()),\n",
    "    StructField(\"name_cash_loan_purpose\", StringType()),\n",
    "    StructField(\"name_contract_status\", StringType()),\n",
    "    StructField(\"days_decision\", IntegerType()),\n",
    "    StructField(\"name_payment_type\", StringType()),\n",
    "    StructField(\"code_reject_reason\", StringType()),\n",
    "    StructField(\"name_type_suite\", StringType()),\n",
    "    StructField(\"name_client_type\", StringType()),\n",
    "    StructField(\"name_goods_category\", StringType()),\n",
    "    StructField(\"name_portfolio\", StringType()),\n",
    "    StructField(\"name_product_type\", StringType()),\n",
    "    StructField(\"channel_type\", StringType()),\n",
    "    StructField(\"sellerplace_area\", IntegerType()),\n",
    "    StructField(\"name_seller_industry\", StringType()),\n",
    "    StructField(\"cnt_payment\", FloatType()),\n",
    "    StructField(\"name_yield_group\", StringType()),\n",
    "    StructField(\"product_combination\", StringType()),\n",
    "    StructField(\"days_first_drawing\", FloatType()),\n",
    "    StructField(\"days_first_due\", FloatType()),\n",
    "    StructField(\"days_last_due_1st_version\", FloatType()),\n",
    "    StructField(\"days_last_due\", FloatType()),\n",
    "    StructField(\"days_termination\", FloatType()),\n",
    "    StructField(\"nflag_insured_on_approval\", FloatType()),\n",
    "    StructField(\"id\", LongType())\n",
    "])\n",
    "\n",
    "value_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"key\", StringType()),\n",
    "    StructField(\"value\", IntegerType()),\n",
    "])\n",
    "\n",
    "# Load CSV files\n",
    "pre_app_df = spark.read.schema(pre_app_schema).csv('previous_application.csv', header = True)\n",
    "value_df = spark.read.schema(value_schema).csv('value_dict.csv', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using the Kafka topic from the producer in Task 1, read the streaming data with Spark Streaming, assuming all data comes in the String format. Except for the 'ts' column, you shall receive it as an Int type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hostip = 'kafka'\n",
    "topic = 'application_stream'\n",
    "\n",
    "app_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\")\\\n",
    "    .load()\n",
    "\n",
    "# Get value of the kafka message\n",
    "app_lines = app_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = app_lines\\\n",
    "        #.writeStream\\\n",
    "        #.outputMode('append')\\\n",
    "        #.format('console')\\\n",
    "        #.trigegr(processingTime = '10 seconds'))\\\n",
    "        #.start(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Then, transform the streaming data format into proper types following the metadata file schema, similar to assignment 2A. Perform the following tasks:  \n",
    "a) For the 'ts' column, convert it to the timestamp format, we will use it as event_time.  \n",
    "b) If the data is late for more than 1 minute, discard it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import explode, from_json, col\n",
    "\n",
    "#Define the schema for the structured datastream received\n",
    "schema = ArrayType(StructType([    \n",
    "    StructField(\"id_app\", StringType(), True),\n",
    "    StructField(\"target\", StringType(), True),\n",
    "    StructField(\"contract_type\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"own_car\", StringType(), True),\n",
    "    StructField(\"own_property\", StringType(), True),\n",
    "    StructField(\"num_of_children\", StringType(), True),\n",
    "    StructField(\"income_total\", StringType(), True),\n",
    "    StructField(\"amt_credit\", StringType(), True),\n",
    "    StructField(\"amt_annuity\", StringType(), True),\n",
    "    StructField(\"amt_goods_price\", StringType(), True),\n",
    "    StructField(\"income_type\", StringType(), True),\n",
    "    StructField(\"education_type\", StringType(), True),\n",
    "    StructField(\"family_status\", StringType(), True),\n",
    "    StructField(\"housing_type\", StringType(), True),\n",
    "    StructField(\"region_population_relative\", StringType(), True),\n",
    "    StructField(\"days_birth\", StringType(), True),\n",
    "    StructField(\"days_employed\", StringType(), True),\n",
    "    StructField(\"own_car_age\", StringType(), True),\n",
    "    StructField(\"flag_mobile\", StringType(), True),\n",
    "    StructField(\"flag_emp_phone\", StringType(), True),\n",
    "    StructField(\"flag_work_phone\", StringType(), True),\n",
    "    StructField(\"flag_cont_mobile\", StringType(), True),\n",
    "    StructField(\"flag_phone\", StringType(), True),\n",
    "    StructField(\"flag_email\", StringType(), True),\n",
    "    StructField(\"occupation_type\", StringType(), True),\n",
    "    StructField(\"cnt_fam_members\", StringType(), True),\n",
    "    StructField(\"weekday_app_process_start\", StringType(), True),\n",
    "    StructField(\"hour_app_process_start\", StringType(), True),\n",
    "    StructField(\"organization_type\", StringType(), True),\n",
    "    StructField(\"credit_score_1\", StringType(), True),\n",
    "    StructField(\"credit_score_2\", StringType(), True),\n",
    "    StructField(\"credit_score_3\", StringType(), True),\n",
    "    StructField(\"days_last_phone_change\", StringType(), True),\n",
    "    StructField(\"amt_credit_req_last_hour\", StringType(), True),\n",
    "    StructField(\"amt_credit_req_last_day\", StringType(), True),\n",
    "    StructField(\"amt_credit_req_last_week\", StringType(), True),\n",
    "    StructField(\"amt_credit_req_last_month\", StringType(), True),\n",
    "    StructField(\"amt_credit_req_last_quarter\", StringType(), True),\n",
    "    StructField(\"amt_credit_req_last_year\", StringType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)\n",
    "]))\n",
    "\n",
    "\n",
    "app_df_1 = app_lines.select(F.from_json(F.col(\"value\").cast(StringType()), schema).alias('parsed_value'))\n",
    "#app_df_1.printSchema()\n",
    "app_df_2 = app_df_1.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))  \n",
    "#app_df_2.printSchema()\n",
    "\n",
    "app_df_formatted = app_df_2.select(\n",
    "                    F.col(\"unnested_value.id_app\").alias(\"id_app\"),\n",
    "                    F.col(\"unnested_value.target\").alias(\"target\"),\n",
    "                    F.col(\"unnested_value.contract_type\").alias(\"contract_type\"),\n",
    "                    F.col(\"unnested_value.gender\").alias(\"gender\"),\n",
    "                    F.col(\"unnested_value.own_car\").alias(\"own_car\"),\n",
    "                    F.col(\"unnested_value.own_property\").alias(\"own_property\"),\n",
    "                    F.col(\"unnested_value.num_of_children\").alias(\"num_of_children\"),\n",
    "                    F.col(\"unnested_value.income_total\").alias(\"income_total\"),\n",
    "                    F.col(\"unnested_value.amt_credit\").alias(\"amt_credit\"),\n",
    "                    F.col(\"unnested_value.amt_annuity\").alias(\"amt_annuity\"),\n",
    "                    F.col(\"unnested_value.amt_goods_price\").alias(\"amt_goods_price\"),\n",
    "                    F.col(\"unnested_value.income_type\").alias(\"income_type\"),\n",
    "                    F.col(\"unnested_value.education_type\").alias(\"education_type\"),\n",
    "                    F.col(\"unnested_value.family_status\").alias(\"family_status\"),\n",
    "                    F.col(\"unnested_value.housing_type\").alias(\"housing_type\"),\n",
    "                    F.col(\"unnested_value.region_population_relative\").alias(\"region_population_relative\"),\n",
    "                    F.col(\"unnested_value.days_birth\").alias(\"days_birth\"),\n",
    "                    F.col(\"unnested_value.days_employed\").alias(\"days_employed\"),\n",
    "                    F.col(\"unnested_value.own_car_age\").alias(\"own_car_age\"),\n",
    "                    F.col(\"unnested_value.flag_mobile\").alias(\"flag_mobile\"),\n",
    "                    F.col(\"unnested_value.flag_emp_phone\").alias(\"flag_emp_phone\"),\n",
    "                    F.col(\"unnested_value.flag_work_phone\").alias(\"flag_work_phone\"),\n",
    "                    F.col(\"unnested_value.flag_cont_mobile\").alias(\"flag_cont_mobile\"),\n",
    "                    F.col(\"unnested_value.flag_phone\").alias(\"flag_phone\"),\n",
    "                    F.col(\"unnested_value.flag_email\").alias(\"flag_email\"),\n",
    "                    F.col(\"unnested_value.occupation_type\").alias(\"occupation_type\"),\n",
    "                    F.col(\"unnested_value.weekday_app_process_start\").alias(\"weekday_appr_process_start\"),\n",
    "                    F.col(\"unnested_value.hour_app_process_start\").alias(\"hour_app_process_start\"),\n",
    "                    F.col(\"unnested_value.organization_type\").alias(\"organization_type\"),\n",
    "                    F.col(\"unnested_value.credit_score_1\").alias(\"credit_score_1\"),\n",
    "                    F.col(\"unnested_value.credit_score_2\").alias(\"credit_score_2\"),\n",
    "                    F.col(\"unnested_value.credit_score_3\").alias(\"credit_score_3\"),\n",
    "                    F.col(\"unnested_value.days_last_phone_change\").alias(\"days_last_phone_change\"),\n",
    "                    F.col(\"unnested_value.amt_credit_req_last_hour\").alias(\"amt_credit_req_last_hour\"),\n",
    "                    F.col(\"unnested_value.amt_credit_req_last_day\").alias(\"amt_credit_req_last_day\"),\n",
    "                    F.col(\"unnested_value.amt_credit_req_last_week\").alias(\"amt_credit_req_last_week\"),\n",
    "                    F.col(\"unnested_value.amt_credit_req_last_month\").alias(\"amt_credit_req_last_month\"),\n",
    "                    F.col(\"unnested_value.amt_credit_req_last_quarter\").alias(\"amt_credit_req_last_quarter\"),\n",
    "                    F.col(\"unnested_value.amt_credit_req_last_year\").alias(\"amt_credit_req_last_year\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
    ")\n",
    "\n",
    "# Transform the datatype\n",
    "app_df_formatted = app_df_formatted.withColumn(\"id_app\", F.col(\"id_app\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"target\", F.col(\"target\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"contract_type\", F.col(\"contract_type\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"num_of_children\", F.col(\"num_of_children\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"income_total\", F.col(\"income_total\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"amt_credit\", F.col(\"amt_credit\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"amt_annuity\", F.col(\"amt_annuity\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"amt_goods_price\", F.col(\"amt_goods_price\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"income_type\", F.col(\"income_type\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"education_type\", F.col(\"education_type\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"family_status\", F.col(\"family_status\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"housing_type\", F.col(\"housing_type\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"region_population_relative\", F.col(\"region_population_relative\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"days_birth\", F.col(\"days_birth\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"days_employed\", F.col(\"days_employed\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"own_car_age\", F.col(\"own_car_age\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"flag_mobile\", F.col(\"flag_mobile\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"flag_emp_phone\", F.col(\"flag_emp_phone\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"flag_work_phone\", F.col(\"flag_work_phone\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"flag_cont_mobile\", F.col(\"flag_cont_mobile\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"flag_phone\", F.col(\"flag_phone\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"flag_email\", F.col(\"flag_email\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"occupation_type\", F.col(\"occupation_type\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"hour_app_process_start\", F.col(\"hour_app_process_start\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"organization_type\", F.col(\"organization_type\").cast(IntegerType()))\\\n",
    "                                    .withColumn(\"credit_score_1\", F.col(\"credit_score_1\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"credit_score_2\", F.col(\"credit_score_2\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"credit_score_3\", F.col(\"credit_score_3\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"days_last_phone_change\", F.col(\"days_last_phone_change\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"amt_credit_req_last_hour\", F.col(\"amt_credit_req_last_hour\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"amt_credit_req_last_day\", F.col(\"amt_credit_req_last_day\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"amt_credit_req_last_week\", F.col(\"amt_credit_req_last_week\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"amt_credit_req_last_month\", F.col(\"amt_credit_req_last_month\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"amt_credit_req_last_quarter\", F.col(\"amt_credit_req_last_quarter\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"amt_credit_req_last_year\", F.col(\"amt_credit_req_last_year\").cast(FloatType()))\\\n",
    "                                    .withColumn(\"ts\", F.col(\"ts\"))\n",
    "\n",
    "app_df_formatted = app_df_formatted.withWatermark(\"ts\", \"1 minute\")\n",
    "#app_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Join the static data frames with the streaming data frame, perform data type/column conversion according to your ML model and print out the Schema. (Again, you can reuse code from 2A).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- own_car: string (nullable = true)\n",
      " |-- own_property: string (nullable = true)\n",
      " |-- amt_credit: float (nullable = true)\n",
      " |-- income_type: string (nullable = true)\n",
      " |-- education_type: string (nullable = true)\n",
      " |-- family_status: string (nullable = true)\n",
      " |-- housing_type: string (nullable = true)\n",
      " |-- flag_emp_phone: integer (nullable = true)\n",
      " |-- occupation_type: string (nullable = true)\n",
      " |-- amt_credit_req_last_year: float (nullable = false)\n",
      " |-- age_bucket: string (nullable = true)\n",
      " |-- loan_to_income_ratio: double (nullable = true)\n",
      " |-- credit_worthiness: string (nullable = true)\n",
      " |-- total_credit: double (nullable = true)\n",
      " |-- total_credit_to_income_ratio: double (nullable = true)\n",
      " |-- work_year: string (nullable = true)\n",
      " |-- num_of_prev_app: long (nullable = false)\n",
      " |-- num_of_approved_app: long (nullable = false)\n",
      " |-- changephone_year: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "def age_calculate(days_birth):\n",
    "    age = math.floor(days_birth/-365)\n",
    "    return age\n",
    "\n",
    "def classify_age(age):\n",
    "    if age < 25:\n",
    "        return 'Y'\n",
    "    elif 25 <= age < 35:\n",
    "        return 'E'\n",
    "    elif 35 <= age < 45:\n",
    "        return 'M'\n",
    "    elif 45 <= age < 55:\n",
    "        return 'L'\n",
    "    elif 55 <= age < 65:\n",
    "        return 'N'\n",
    "    elif age >= 65:\n",
    "        return 'R'\n",
    "\n",
    "def set_credit_worthiness(avg_score):\n",
    "    if avg_score >= 0.7:\n",
    "        return 'high'\n",
    "    elif 0.4 <= avg_score < 0.7:\n",
    "        return 'medium'\n",
    "    elif avg_score < 0.4:\n",
    "        return 'low'\n",
    "\n",
    "def workyear_calculate(days_employed):\n",
    "    workyear = days_employed/-365\n",
    "    return workyear\n",
    "\n",
    "def classify_workyear(workyear):\n",
    "    if 0 < workyear < 1:\n",
    "        return '< 1'\n",
    "    elif 1 <= workyear <= 10:\n",
    "        return '1-10'\n",
    "    elif 10 < workyear <= 20:\n",
    "        return '11-20'\n",
    "    elif 20 < workyear <= 30:\n",
    "        return '21-30'\n",
    "    elif 30 < workyear <= 40:\n",
    "        return '31-40'\n",
    "    elif 40 < workyear <= 50:\n",
    "        return '41-50'\n",
    "    elif 50 < workyear <= 60:\n",
    "        return '51-60'\n",
    "    elif workyear < 0 or workyear == 0:\n",
    "        return '0'\n",
    "\n",
    "def changephone_calculate(days_last_phone_change):\n",
    "    if days_last_phone_change is not None:\n",
    "        changeyear = days_last_phone_change / -365\n",
    "        return changeyear\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def classify_changephone(changeyear):\n",
    "    if 0 < changeyear < 1:\n",
    "        return '< 1'\n",
    "    elif 1 <= changeyear <= 5:\n",
    "        return '1-5'\n",
    "    elif 5 < changeyear <= 10:\n",
    "        return '6-10'\n",
    "    elif 10 < changeyear <= 15:\n",
    "        return '11-15'\n",
    "    elif 15 < changeyear <= 20:\n",
    "        return '16-20'\n",
    "    else:\n",
    "        return '0'\n",
    "    \n",
    "def replace_value(df, column, value_df):  \n",
    "    df = df.join(value_df, F.col(column) == value_df.value, how = 'inner')\n",
    "    df = df.withColumn(column, F.col('key'))\n",
    "    df = df.drop('value').drop('key')\n",
    "    return df\n",
    "        \n",
    "# Register the functions as UDFs\n",
    "age_calculate_udf = udf(age_calculate, IntegerType())\n",
    "classify_age_udf = udf(classify_age, StringType())\n",
    "set_credit_worthiness_udf = udf(set_credit_worthiness, StringType())\n",
    "workyear_calculate_udf = udf(workyear_calculate, IntegerType())\n",
    "classify_workyear_udf = udf(classify_workyear, StringType())\n",
    "changephone_calculate_udf = udf(changephone_calculate, IntegerType())\n",
    "classify_changephone_udf = udf(classify_changephone, StringType())\n",
    "\n",
    "# Add age_bucket and loan_to_income_ratio columns\n",
    "app_df_new = app_df_formatted.withColumn(\"age_bucket\", classify_age_udf(age_calculate_udf(F.col('days_birth'))))\\\n",
    "                .withColumn('loan_to_income_ratio', F.col('amt_credit') / F.col('income_total'))\n",
    "    \n",
    "# replace null value with 0.5\n",
    "columns = ['credit_score_1', 'credit_score_2', 'credit_score_3']\n",
    "for col in columns:\n",
    "    app_df_new = app_df_new.withColumn(col, F.coalesce(col, F.lit(0.5)))\n",
    "\n",
    "# Add credit_worthiness column # Add work_year and changephone_year\n",
    "app_df_new = app_df_new.withColumn('credit_worthiness', set_credit_worthiness_udf((F.col('credit_score_1') + F.col('credit_score_2') + F.col('credit_score_3')) / 3))\\\n",
    "                                .withColumn(\"work_year\", classify_workyear_udf(workyear_calculate_udf(F.col('days_employed'))))\\\n",
    "                                .withColumn(\"changephone_year\", classify_changephone_udf(changephone_calculate_udf(F.col('days_last_phone_change'))))\n",
    "\n",
    "## Join app_df and pre_app_df\n",
    "pre_app_df_renamed = pre_app_df.withColumnRenamed('id_app', 'id_app_1')\\\n",
    "                                .withColumnRenamed('amt_credit', 'amt_credit_1')\n",
    "\n",
    "# join application_data with previous_application\n",
    "joined_df = app_df_new.join(pre_app_df_renamed, app_df_new.id_app == pre_app_df_renamed.id_app_1, how = 'left_outer')\n",
    "\n",
    "# Create 'num_of_prev_app' column\n",
    "joined_df_count_pre = joined_df.groupby('id_app').agg(F.count('id').alias('num_of_prev_app'))\n",
    "\n",
    "# Rename column to avoid duplicate column name\n",
    "joined_df_count_pre = joined_df_count_pre.withColumnRenamed('id_app', 'id_app_2')\n",
    "\n",
    "app_df_1 = app_df_new.join(joined_df_count_pre, expr(\"\"\"id_app == id_app_2\"\"\"), how = 'inner')\\\n",
    "                    .drop('id_app_2')\n",
    "\n",
    "# Create 'num_of_approved_app' column\n",
    "joined_df_count_approved = joined_df.filter(F.col('name_contract_status') == 'Approved').groupby('id_app')\\\n",
    "                                    .agg(F.count('id').alias('num_of_approved_app'))\n",
    "\n",
    "# Rename column to avoid duplicate column name\n",
    "joined_df_count_approved = joined_df_count_approved.withColumnRenamed('id_app', 'id_app_2')\n",
    "\n",
    "app_df_2 = app_df_1.join(joined_df_count_approved, expr(\"\"\"id_app == id_app_2\"\"\"), how = 'inner')\\\n",
    "                    .drop('id_app_2')\n",
    "\n",
    "# Replace null value with 0\n",
    "app_df_2 = app_df_2.withColumn('num_of_approved_app', F.when(F.col('num_of_approved_app').isNull(), 0).otherwise(F.col('num_of_approved_app')))\n",
    "\n",
    "# Create 'total_credit' column\n",
    "joined_df_total = joined_df.filter(F.col('name_contract_status') == 'Approved').groupBy('id_app')\\\n",
    "                            .agg(F.sum('amt_credit_1').alias('total_credit'))\n",
    "\n",
    "# Rename column to avoid duplicate column name\n",
    "joined_df_total = joined_df_total.withColumnRenamed('id_app', 'id_app_2')\n",
    "\n",
    "app_df_3 = app_df_2.join(joined_df_total, expr(\"\"\"id_app == id_app_2\"\"\"), how = 'inner')\\\n",
    "                    .drop('id_app_2')\n",
    "\n",
    "# Replace null value with 0\n",
    "app_df_3 = app_df_3.withColumn('total_credit', F.when(F.col('total_credit').isNull(), 0).otherwise(F.col('total_credit')))\n",
    "\n",
    "# Create 'total_credit_to_income_ratio' column\n",
    "app_df_4 = app_df_3.withColumn('total_credit_to_income_ratio', F.col('total_credit')/F.col('income_total'))\n",
    "\n",
    "# Replace education_type\n",
    "value_df_edu = value_df.filter(F.col('category') == 'education_type').select('value', 'key')\n",
    "app_df_edu = replace_value(app_df_4, 'education_type', value_df_edu)\n",
    "\n",
    "# Replace occupation_type\n",
    "value_df_occ = value_df.filter(F.col('category') == 'occupation_type').select('value', 'key')\n",
    "app_df_occ = replace_value(app_df_edu, 'occupation_type', value_df_occ)\n",
    "\n",
    "# Replace income_type\n",
    "value_df_in = value_df.filter(F.col('category') == 'income_type').select('value', 'key')\n",
    "app_df_in = replace_value(app_df_occ, 'income_type', value_df_in)\n",
    "\n",
    "# Replace family_status\n",
    "value_df_fam = value_df.filter(F.col('category') == 'family_status').select('value', 'key')\n",
    "app_df_fam = replace_value(app_df_in, 'family_status', value_df_fam)   \n",
    "\n",
    "# Replace housing_type\n",
    "value_df_housing = value_df.filter(F.col('category') == 'housing_type').select('value', 'key')\n",
    "app_df_housing = replace_value(app_df_fam, 'housing_type', value_df_housing)\n",
    "\n",
    "# Fill null with 0 for \"amt_credit_req_last_year\"\n",
    "app_df_amt = app_df_housing.fillna(0, subset=['amt_credit_req_last_year'])\n",
    "\n",
    "cols = ['gender', 'own_car', 'own_property', 'amt_credit', 'income_type', 'education_type', 'family_status', 'housing_type', \n",
    "        'flag_emp_phone', 'occupation_type','amt_credit_req_last_year', 'age_bucket', 'loan_to_income_ratio', \n",
    "        'credit_worthiness', 'total_credit', 'total_credit_to_income_ratio', 'work_year', 'num_of_prev_app',\n",
    "        'num_of_approved_app', 'changephone_year', 'ts']\n",
    "\n",
    "app_df_final = app_df_amt.select([x for x in cols]).na.drop()\n",
    "app_df_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Load your ML model, and use the model and Spark to perform the following:  \n",
    "    a) Every 10 seconds, print the total number of applications and number of potential default applications (prediction = 1) in the last 1 minute.  \n",
    "    b) Every 15 seconds, show the total requested credit (sum of credit where default=0) in the last 15 seconds.  \n",
    "    c) Every 1 minute, show the top 10 largest loan applications with the potential of default.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_df_formatted = app_df_formatted.withColumnRenamed('target', 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------+----------------------------------------+\n",
      "|window|total_number_of_applications|number_of_potential_default_applications|\n",
      "+------+----------------------------+----------------------------------------+\n",
      "+------+----------------------------+----------------------------------------+\n",
      "\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|window                                    |total_number_of_applications|number_of_potential_default_applications|\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|{2024-02-07 20:19:20, 2024-02-07 20:20:20}|116                         |6                                       |\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|window                                    |total_number_of_applications|number_of_potential_default_applications|\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|{2024-02-07 20:19:20, 2024-02-07 20:20:20}|116                         |6                                       |\n",
      "|{2024-02-07 20:19:30, 2024-02-07 20:20:30}|700                         |54                                      |\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|window                                    |total_number_of_applications|number_of_potential_default_applications|\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|{2024-02-07 20:19:20, 2024-02-07 20:20:20}|116                         |6                                       |\n",
      "|{2024-02-07 20:19:30, 2024-02-07 20:20:30}|700                         |54                                      |\n",
      "|{2024-02-07 20:19:40, 2024-02-07 20:20:40}|1523                        |131                                     |\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|window                                    |total_number_of_applications|number_of_potential_default_applications|\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|{2024-02-07 20:19:20, 2024-02-07 20:20:20}|116                         |6                                       |\n",
      "|{2024-02-07 20:19:30, 2024-02-07 20:20:30}|700                         |54                                      |\n",
      "|{2024-02-07 20:19:40, 2024-02-07 20:20:40}|1523                        |131                                     |\n",
      "|{2024-02-07 20:19:50, 2024-02-07 20:20:50}|2329                        |200                                     |\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|window                                    |total_number_of_applications|number_of_potential_default_applications|\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|{2024-02-07 20:19:20, 2024-02-07 20:20:20}|116                         |6                                       |\n",
      "|{2024-02-07 20:19:30, 2024-02-07 20:20:30}|700                         |54                                      |\n",
      "|{2024-02-07 20:19:40, 2024-02-07 20:20:40}|1523                        |131                                     |\n",
      "|{2024-02-07 20:19:50, 2024-02-07 20:20:50}|2329                        |200                                     |\n",
      "|{2024-02-07 20:20:00, 2024-02-07 20:21:00}|2830                        |236                                     |\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|window                                    |total_number_of_applications|number_of_potential_default_applications|\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|{2024-02-07 20:19:20, 2024-02-07 20:20:20}|116                         |6                                       |\n",
      "|{2024-02-07 20:19:30, 2024-02-07 20:20:30}|700                         |54                                      |\n",
      "|{2024-02-07 20:19:40, 2024-02-07 20:20:40}|1523                        |131                                     |\n",
      "|{2024-02-07 20:19:50, 2024-02-07 20:20:50}|2329                        |200                                     |\n",
      "|{2024-02-07 20:20:00, 2024-02-07 20:21:00}|2830                        |236                                     |\n",
      "|{2024-02-07 20:20:10, 2024-02-07 20:21:10}|3169                        |254                                     |\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|window                                    |total_number_of_applications|number_of_potential_default_applications|\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|{2024-02-07 20:19:30, 2024-02-07 20:20:30}|700                         |54                                      |\n",
      "|{2024-02-07 20:19:40, 2024-02-07 20:20:40}|1523                        |131                                     |\n",
      "|{2024-02-07 20:19:50, 2024-02-07 20:20:50}|2329                        |200                                     |\n",
      "|{2024-02-07 20:20:00, 2024-02-07 20:21:00}|2830                        |236                                     |\n",
      "|{2024-02-07 20:20:10, 2024-02-07 20:21:10}|3169                        |254                                     |\n",
      "|{2024-02-07 20:20:20, 2024-02-07 20:21:20}|3574                        |291                                     |\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|window                                    |total_number_of_applications|number_of_potential_default_applications|\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "|{2024-02-07 20:19:40, 2024-02-07 20:20:40}|1523                        |131                                     |\n",
      "|{2024-02-07 20:19:50, 2024-02-07 20:20:50}|2329                        |200                                     |\n",
      "|{2024-02-07 20:20:00, 2024-02-07 20:21:00}|2830                        |236                                     |\n",
      "|{2024-02-07 20:20:10, 2024-02-07 20:21:10}|3169                        |254                                     |\n",
      "|{2024-02-07 20:20:20, 2024-02-07 20:21:20}|3574                        |291                                     |\n",
      "|{2024-02-07 20:20:30, 2024-02-07 20:21:30}|3459                        |283                                     |\n",
      "+------------------------------------------+----------------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6a\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "def show_result(df, epoch_id):\n",
    "    df.show(truncate = False)\n",
    "    \n",
    "window_count_6a = app_df_formatted \\\n",
    "    .withWatermark('ts', '1 minute') \\\n",
    "    .groupBy(F.window('ts', '1 minute', '10 seconds')) \\\n",
    "    .agg(F.count('*').alias('total_number_of_applications'), \n",
    "        F.count(expr(\"CASE WHEN prediction = 1 THEN 1 END\")).alias('number_of_potential_default_applications'))\\\n",
    "    .filter((F.col('window.end') >= current_timestamp() - F.expr('INTERVAL 1 MINUTE')) & \n",
    "            (F.col('window.end') <= current_timestamp())) \\\n",
    "    .orderBy('window')\n",
    "    \n",
    "query_6a = window_count_6a \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(show_result)\\\n",
    "    .trigger(processingTime = '10 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_6a.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+\n",
      "|window|total_requested_credit|\n",
      "+------+----------------------+\n",
      "+------+----------------------+\n",
      "\n",
      "+------------------------------------------+----------------------+\n",
      "|window                                    |total_requested_credit|\n",
      "+------------------------------------------+----------------------+\n",
      "|{2024-02-07 20:21:45, 2024-02-07 20:22:00}|182616683             |\n",
      "+------------------------------------------+----------------------+\n",
      "\n",
      "+------------------------------------------+----------------------+\n",
      "|window                                    |total_requested_credit|\n",
      "+------------------------------------------+----------------------+\n",
      "|{2024-02-07 20:22:00, 2024-02-07 20:22:15}|287908250             |\n",
      "+------------------------------------------+----------------------+\n",
      "\n",
      "+------------------------------------------+----------------------+\n",
      "|window                                    |total_requested_credit|\n",
      "+------------------------------------------+----------------------+\n",
      "|{2024-02-07 20:22:15, 2024-02-07 20:22:30}|339588392             |\n",
      "+------------------------------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6b\n",
    "from pyspark.sql.functions import sum, format_string\n",
    "    \n",
    "window_count_6b = app_df_formatted \\\n",
    "    .withWatermark('ts', '15 seconds') \\\n",
    "    .filter(F.col('prediction') == 0)\\\n",
    "    .groupBy(F.window('ts', '15 seconds')) \\\n",
    "    .agg(F.format_string('%.0f', F.sum(F.col('amt_credit'))).alias('total_requested_credit')) \\\n",
    "    .filter(F.col('window.end') >= current_timestamp() - F.expr('INTERVAL 15 SECONDS')) \\\n",
    "    .orderBy('window')\n",
    "\n",
    "query_6b = window_count_6b \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(show_result)\\\n",
    "    .trigger(processingTime = '15 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_6b.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+\n",
      "| ts|id_app|amt_credit|rank|\n",
      "+---+------+----------+----+\n",
      "+---+------+----------+----+\n",
      "\n",
      "+-------------------+------+----------+----+\n",
      "|                 ts|id_app|amt_credit|rank|\n",
      "+-------------------+------+----------+----+\n",
      "|2024-02-07 20:23:30|378362| 2281500.0|   1|\n",
      "|2024-02-07 20:23:10|376463| 1800000.0|   2|\n",
      "|2024-02-07 20:23:05|375998| 1762110.0|   3|\n",
      "|2024-02-07 20:23:50|380052| 1585224.0|   4|\n",
      "|2024-02-07 20:23:35|378857| 1546020.0|   5|\n",
      "|2024-02-07 20:23:55|380454| 1546020.0|   5|\n",
      "|2024-02-07 20:23:20|377497| 1512796.5|   7|\n",
      "|2024-02-07 20:23:50|379954| 1436850.0|   8|\n",
      "|2024-02-07 20:23:35|378797| 1423584.0|   9|\n",
      "|2024-02-07 20:23:25|377856| 1314000.0|  10|\n",
      "+-------------------+------+----------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------+------+----------+----+\n",
      "|                 ts|id_app|amt_credit|rank|\n",
      "+-------------------+------+----------+----+\n",
      "|2024-02-07 20:24:46|384200| 1724688.0|   1|\n",
      "|2024-02-07 20:24:20|382272| 1575000.0|   2|\n",
      "|2024-02-07 20:24:41|383702| 1575000.0|   2|\n",
      "|2024-02-07 20:24:31|383097| 1546020.0|   4|\n",
      "|2024-02-07 20:24:36|383658| 1525482.0|   5|\n",
      "|2024-02-07 20:24:31|382705| 1506816.0|   6|\n",
      "|2024-02-07 20:24:05|381101| 1494688.5|   7|\n",
      "|2024-02-07 20:24:51|384716| 1470330.0|   8|\n",
      "|2024-02-07 20:24:46|384094| 1317357.0|   9|\n",
      "|2024-02-07 20:24:36|383521| 1303812.0|  10|\n",
      "+-------------------+------+----------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------+------+----------+----+\n",
      "|                 ts|id_app|amt_credit|rank|\n",
      "+-------------------+------+----------+----+\n",
      "|2024-02-07 20:25:46|388736| 2097058.5|   1|\n",
      "|2024-02-07 20:25:01|385667| 2085120.0|   2|\n",
      "|2024-02-07 20:25:31|387693| 2032992.0|   3|\n",
      "|2024-02-07 20:25:51|388976| 2013840.0|   4|\n",
      "|2024-02-07 20:25:01|385575| 1983631.5|   5|\n",
      "|2024-02-07 20:25:21|387013| 1963494.0|   6|\n",
      "|2024-02-07 20:25:41|388404| 1749325.5|   7|\n",
      "|2024-02-07 20:25:46|388778| 1636245.0|   8|\n",
      "|2024-02-07 20:25:26|387615| 1506816.0|   9|\n",
      "|2024-02-07 20:25:11|386051| 1473871.5|  10|\n",
      "+-------------------+------+----------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window().orderBy(F.col(\"amt_credit\").desc())\n",
    "\n",
    "# The function ranks amt_credit\n",
    "def rank_loan(df, epoch_id):\n",
    "    sorted_df = df.withColumn('rank', F.rank().over(window_spec))\\\n",
    "                    .show(10)\n",
    "                    \n",
    "window_rank_6c = app_df_formatted\\\n",
    "    .withWatermark('ts', '1 minute')\\\n",
    "    .filter(F.col('prediction') == 1)\\\n",
    "    .select('ts', 'id_app', 'amt_credit')\n",
    "  \n",
    "query_6c = window_rank_6c\\\n",
    "    .writeStream\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .foreachBatch(rank_loan)\\\n",
    "    .trigger(processingTime = '1 minute')\\\n",
    "    .option(\"truncate\", \"False\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_6c.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write a Parquet file to store the following data:  \n",
    "    a) Persist the raw data: Persist your prediction results along with application data and event_time in Parquet format; after that, read the Parquet file and show the first 10 records.  \n",
    "    b) Persist the 15-second requested credit (6b) in another parquet file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7a\n",
    "query_app = app_df_formatted\\\n",
    "        .writeStream\\\n",
    "        .format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/app_formatted_df\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/app_formatted_df/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_app.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_app: integer (nullable = true)\n",
      " |-- prediction: integer (nullable = true)\n",
      " |-- contract_type: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- own_car: string (nullable = true)\n",
      " |-- own_property: string (nullable = true)\n",
      " |-- num_of_children: integer (nullable = true)\n",
      " |-- income_total: float (nullable = true)\n",
      " |-- amt_credit: float (nullable = true)\n",
      " |-- amt_annuity: float (nullable = true)\n",
      " |-- amt_goods_price: float (nullable = true)\n",
      " |-- income_type: integer (nullable = true)\n",
      " |-- education_type: integer (nullable = true)\n",
      " |-- family_status: integer (nullable = true)\n",
      " |-- housing_type: integer (nullable = true)\n",
      " |-- region_population_relative: float (nullable = true)\n",
      " |-- days_birth: integer (nullable = true)\n",
      " |-- days_employed: integer (nullable = true)\n",
      " |-- own_car_age: float (nullable = true)\n",
      " |-- flag_mobile: integer (nullable = true)\n",
      " |-- flag_emp_phone: integer (nullable = true)\n",
      " |-- flag_work_phone: integer (nullable = true)\n",
      " |-- flag_cont_mobile: integer (nullable = true)\n",
      " |-- flag_phone: integer (nullable = true)\n",
      " |-- flag_email: integer (nullable = true)\n",
      " |-- occupation_type: integer (nullable = true)\n",
      " |-- weekday_appr_process_start: string (nullable = true)\n",
      " |-- hour_app_process_start: integer (nullable = true)\n",
      " |-- organization_type: integer (nullable = true)\n",
      " |-- credit_score_1: float (nullable = true)\n",
      " |-- credit_score_2: float (nullable = true)\n",
      " |-- credit_score_3: float (nullable = true)\n",
      " |-- days_last_phone_change: float (nullable = true)\n",
      " |-- amt_credit_req_last_hour: float (nullable = true)\n",
      " |-- amt_credit_req_last_day: float (nullable = true)\n",
      " |-- amt_credit_req_last_week: float (nullable = true)\n",
      " |-- amt_credit_req_last_month: float (nullable = true)\n",
      " |-- amt_credit_req_last_quarter: float (nullable = true)\n",
      " |-- amt_credit_req_last_year: float (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n",
      "+------+----------+-------------+------+-------+------------+---------------+------------+----------+-----------+---------------+-----------+--------------+-------------+------------+--------------------------+----------+-------------+-----------+-----------+--------------+---------------+----------------+----------+----------+---------------+--------------------------+----------------------+-----------------+--------------+--------------+--------------+----------------------+------------------------+-----------------------+------------------------+-------------------------+---------------------------+------------------------+-------------------+\n",
      "|id_app|prediction|contract_type|gender|own_car|own_property|num_of_children|income_total|amt_credit|amt_annuity|amt_goods_price|income_type|education_type|family_status|housing_type|region_population_relative|days_birth|days_employed|own_car_age|flag_mobile|flag_emp_phone|flag_work_phone|flag_cont_mobile|flag_phone|flag_email|occupation_type|weekday_appr_process_start|hour_app_process_start|organization_type|credit_score_1|credit_score_2|credit_score_3|days_last_phone_change|amt_credit_req_last_hour|amt_credit_req_last_day|amt_credit_req_last_week|amt_credit_req_last_month|amt_credit_req_last_quarter|amt_credit_req_last_year|                 ts|\n",
      "+------+----------+-------------+------+-------+------------+---------------+------------+----------+-----------+---------------+-----------+--------------+-------------+------------+--------------------------+----------+-------------+-----------+-----------+--------------+---------------+----------------+----------+----------+---------------+--------------------------+----------------------+-----------------+--------------+--------------+--------------+----------------------+------------------------+-----------------------+------------------------+-------------------------+---------------------------+------------------------+-------------------+\n",
      "|391474|         0|            2|     F|      N|           Y|              0|    135000.0| 1293502.5|    37948.5|      1129500.0|          5|             1|            3|           6|                      NULL|    -20794|       365243|       NULL|          1|             0|              0|               1|         0|         0|             18|                  THURSDAY|                    13|               31|          NULL|    0.60056937|     0.4311918|                   0.0|                     0.0|                    0.0|                     0.0|                      0.0|                        0.0|                     1.0|2024-02-07 20:26:31|\n",
      "|391475|         0|            2|     F|      N|           N|              0|    112500.0|  247500.0|    18504.0|       247500.0|          2|             1|            2|           6|                      NULL|    -16415|        -5482|       NULL|          1|             1|              1|               1|         0|         0|             16|                 WEDNESDAY|                    10|               50|     0.8306324|    0.30738503|     0.4311918|               -1667.0|                     0.0|                    0.0|                     0.0|                      0.0|                        0.0|                     4.0|2024-02-07 20:26:31|\n",
      "|391476|         0|            2|     F|      N|           Y|              1|    112500.0|  755190.0|    29947.5|       675000.0|          2|             1|            4|           6|                      NULL|    -16430|        -1997|       NULL|          1|             1|              1|               1|         0|         0|             16|                   TUESDAY|                     9|               42|    0.47126853|    0.49874997|     0.7267112|                   0.0|                     0.0|                    0.0|                     0.0|                      2.0|                        0.0|                     0.0|2024-02-07 20:26:31|\n",
      "|391477|         1|            2|     F|      N|           N|              1|    157500.0|  450000.0|    25128.0|       450000.0|          2|             1|            3|           6|                      NULL|    -13589|         -817|       NULL|          1|             1|              1|               1|         0|         0|             12|                    SUNDAY|                    12|               36|     0.4329569|     0.4697577|     0.7463002|                -651.0|                     0.0|                    0.0|                     0.0|                      0.0|                        1.0|                     0.0|2024-02-07 20:26:31|\n",
      "|391478|         0|            1|     M|      N|           Y|              0|    247500.0|  270000.0|    13500.0|       270000.0|          2|             4|            3|           6|                      NULL|    -13703|         -676|       NULL|          1|             1|              0|               1|         0|         0|             12|                    FRIDAY|                     9|               36|    0.44659567|    0.45206276|     0.7801436|                -720.0|                     0.0|                    0.0|                     0.0|                      0.0|                        0.0|                     4.0|2024-02-07 20:26:31|\n",
      "|391479|         0|            1|     M|      Y|           Y|              0|     54000.0|  270000.0|    13500.0|       270000.0|          5|             1|            3|           6|                      NULL|    -21931|       365243|       31.0|          1|             0|              0|               1|         0|         0|             18|                    FRIDAY|                     5|               31|          NULL|    0.48984033|      0.723837|               -1600.0|                     0.0|                    0.0|                     0.0|                      0.0|                        0.0|                     0.0|2024-02-07 20:26:31|\n",
      "|391480|         0|            2|     F|      N|           Y|              1|    225000.0|  450000.0|    24412.5|       450000.0|          2|             1|            2|           6|                      NULL|     -9082|        -2230|       NULL|          1|             1|              0|               1|         0|         0|             12|                   TUESDAY|                    15|               36|          NULL|     0.6067586|    0.41010258|                -817.0|                     0.0|                    0.0|                     0.0|                      0.0|                        0.0|                     1.0|2024-02-07 20:26:31|\n",
      "|391481|         0|            2|     F|      N|           N|              0|     90000.0| 1193580.0|    35028.0|       855000.0|          2|             1|            3|           6|                      NULL|    -15563|        -4654|       NULL|          1|             1|              0|               1|         0|         0|             16|                  THURSDAY|                     7|               50|     0.5994412|    0.41898665|    0.70470643|               -2545.0|                     0.0|                    0.0|                     0.0|                      1.0|                        0.0|                     2.0|2024-02-07 20:26:31|\n",
      "|391482|         0|            2|     M|      Y|           Y|              1|    157500.0|  761872.5|    58959.0|       675000.0|          2|             1|            3|           6|                      NULL|    -10558|         -506|        7.0|          1|             1|              0|               1|         0|         0|              9|                   TUESDAY|                    16|               50|          NULL|     0.5739314|    0.69409263|                -628.0|                     0.0|                    0.0|                     0.0|                      0.0|                        2.0|                     0.0|2024-02-07 20:26:31|\n",
      "|391483|         0|            2|     M|      Y|           Y|              0|    126000.0|  298512.0|    31801.5|       270000.0|          8|             4|            6|           6|                      NULL|    -10051|        -1157|       17.0|          1|             1|              0|               1|         0|         1|             16|                    SUNDAY|                    11|               50|          NULL|    0.54113454|          NULL|               -1099.0|                    NULL|                   NULL|                    NULL|                     NULL|                       NULL|                    NULL|2024-02-07 20:26:31|\n",
      "+------+----------+-------------+------+-------+------------+---------------+------------+----------+-----------+---------------+-----------+--------------+-------------+------------+--------------------------+----------+-------------+-----------+-----------+--------------+---------------+----------------+----------+----------+---------------+--------------------------+----------------------+-----------------+--------------+--------------+--------------+----------------------+------------------------+-----------------------+------------------------+-------------------------+---------------------------+------------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_app = spark.read.parquet(\"parquet/app_formatted_df\")\n",
    "query_app.printSchema()\n",
    "query_app.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7b\n",
    "def write_df(df, epoch_id):\n",
    "    df.write.mode(\"append\").parquet(\"parquet/total_credit\")\n",
    "    \n",
    "query_7b = window_count_6b\\\n",
    "        .writeStream\\\n",
    "        .foreachBatch(write_df)\\\n",
    "        .outputMode(\"complete\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/total_credit/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_7b.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- total_requested_credit: string (nullable = true)\n",
      "\n",
      "+------------------------------------------+----------------------+\n",
      "|window                                    |total_requested_credit|\n",
      "+------------------------------------------+----------------------+\n",
      "|{2024-02-07 20:28:45, 2024-02-07 20:29:00}|685636929             |\n",
      "|{2024-02-07 20:28:15, 2024-02-07 20:28:30}|627555038             |\n",
      "|{2024-02-07 20:29:00, 2024-02-07 20:29:15}|102343950             |\n",
      "|{2024-02-07 20:28:00, 2024-02-07 20:28:15}|422220402             |\n",
      "|{2024-02-07 20:27:30, 2024-02-07 20:27:45}|419525330             |\n",
      "|{2024-02-07 20:28:15, 2024-02-07 20:28:30}|627555038             |\n",
      "|{2024-02-07 20:28:30, 2024-02-07 20:28:45}|275397813             |\n",
      "|{2024-02-07 20:27:15, 2024-02-07 20:27:30}|615114050             |\n",
      "|{2024-02-07 20:27:15, 2024-02-07 20:27:30}|161934777             |\n",
      "|{2024-02-07 20:27:00, 2024-02-07 20:27:15}|262658007             |\n",
      "+------------------------------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_7b = spark.read.parquet(\"parquet/total_credit\")\n",
    "query_7b.printSchema()\n",
    "query_7b.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Read the two parquet files from 7a and 7b as a data stream, and send the records to two topics with appropriate names.  \n",
    "(Note: You shall read the parquet files as a streaming data frame and send messages to the Kafka topic when new data appears in the parquet files. The parquet files serve as an intermediate storage in this use case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stream 1\n",
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "hostip = 'kafka'\n",
    "topic = 'application_pred_stream'\n",
    "\n",
    "parquet_schema = StructType([\n",
    "    StructField(\"id_app\", IntegerType(), True),\n",
    "    StructField(\"prediction\", IntegerType(), True),\n",
    "    StructField(\"contract_type\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"own_car\", StringType(), True),\n",
    "    StructField(\"own_property\", StringType(), True),\n",
    "    StructField(\"num_of_children\", IntegerType(), True),\n",
    "    StructField(\"income_total\", FloatType(), True),\n",
    "    StructField(\"amt_credit\", FloatType(), True),\n",
    "    StructField(\"amt_annuity\", FloatType(), True),\n",
    "    StructField(\"amt_goods_price\", FloatType(), True),\n",
    "    StructField(\"income_type\", IntegerType(), True),\n",
    "    StructField(\"education_type\", IntegerType(), True),\n",
    "    StructField(\"family_status\", IntegerType(), True),\n",
    "    StructField(\"housing_type\", IntegerType(), True),\n",
    "    StructField(\"region_population_relative\", FloatType(), True),\n",
    "    StructField(\"days_birth\", IntegerType(), True),\n",
    "    StructField(\"days_employed\", IntegerType(), True),\n",
    "    StructField(\"own_car_age\", FloatType(), True),\n",
    "    StructField(\"flag_mobile\", IntegerType(), True),\n",
    "    StructField(\"flag_emp_phone\", IntegerType(), True),\n",
    "    StructField(\"flag_work_phone\", IntegerType(), True),\n",
    "    StructField(\"flag_cont_mobile\", IntegerType(), True),\n",
    "    StructField(\"flag_phone\", IntegerType(), True),\n",
    "    StructField(\"flag_email\", IntegerType(), True),\n",
    "    StructField(\"occupation_type\", IntegerType(), True),\n",
    "    StructField(\"weekday_appr_process_start\", StringType(), True),\n",
    "    StructField(\"hour_app_process_start\", IntegerType(), True),\n",
    "    StructField(\"organization_type\", IntegerType(), True),\n",
    "    StructField(\"credit_score_1\", FloatType(), True),\n",
    "    StructField(\"credit_score_2\", FloatType(), True),\n",
    "    StructField(\"credit_score_3\", FloatType(), True),\n",
    "    StructField(\"days_last_phone_change\", FloatType(), True),\n",
    "    StructField(\"amt_credit_req_last_hour\", FloatType(), True),\n",
    "    StructField(\"amt_credit_req_last_day\", FloatType(), True),\n",
    "    StructField(\"amt_credit_req_last_week\", FloatType(), True),\n",
    "    StructField(\"amt_credit_req_last_month\", FloatType(), True),\n",
    "    StructField(\"amt_credit_req_last_quarter\", FloatType(), True),\n",
    "    StructField(\"amt_credit_req_last_year\", FloatType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "app_pred_df = spark \\\n",
    "    .readStream \\\n",
    "    .schema(parquet_schema)\\\n",
    "    .format(\"parquet\")\\\n",
    "    .option(\"path\", \"parquet/app_formatted_df\")\\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\")\\\n",
    "    .load()      \n",
    "\n",
    "query_8a = app_pred_df \\\n",
    "    .select(to_json(F.struct(\"*\")).alias('value')) \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\")\\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"topic\", topic) \\\n",
    "    .trigger(processingTime = '15 seconds') \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint/app_formatted_df\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_8a.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 2\n",
    "\n",
    "hostip = 'kafka'\n",
    "topic = 'total_credit'\n",
    "\n",
    "parquet_schema = StructType([\n",
    "    StructField(\"window\", StructType([\n",
    "        StructField(\"start\",  TimestampType(), True),\n",
    "        StructField(\"end\",  TimestampType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"total_requested_credit\", StringType(), True),\n",
    "])\n",
    "\n",
    "total_credit = spark \\\n",
    "    .readStream\\\n",
    "    .schema(parquet_schema) \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"parquet/total_credit\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "query_8b = total_credit \\\n",
    "    .select(to_json(F.struct(\"*\")).alias('value')) \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"topic\", topic) \\\n",
    "    .trigger(processingTime='15 seconds') \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint/total_credit\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_8b.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
