{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Producing the data\n",
    "In this task, we will implement one Apache Kafka producer to simulate real-time data streaming. Spark is not allowed/required in this part since it’s simulating a streaming data source.\n",
    "\n",
    "1. Your program should send one batch of applications every 5 seconds. One batch consists of a random 100-500 rows from the application_data_stream dataset. Note that only the number of rows needs to be random, you can read the file sequentially.  \n",
    "    As an example, in the first and second batches, assuming we generate random numbers 100 and 400, the first batch will send records 1-100 from the CSV file, and the second batch will send records 101-500.  \n",
    "    The CSV shouldn’t be loaded to memory at once to conserve memory (i.e. Read rows as needed).  \n",
    "2. Add an integer column named ‘ts’ for each row, a Unix timestamp in seconds since the epoch (UTC timezone). Spead your batch out evenly for 5 seconds.  \n",
    "    For example, if you send a batch of 100 records at 2024-02-01 00:00:00 (ISO format: YYYY-MM-DD HH:MM:SS) -> (ts = 1704027600):\n",
    "    - Record 1-20: ts = 1704027600 \n",
    "    - Record 21-40: ts = 1704027601 \n",
    "    - Record 41-60: ts = 1704027602\n",
    "    - ….\n",
    "3. Send your batch to a Kafka topic with an appropriate name.\n",
    "\n",
    "All the data except for the ‘ts’ column should be sent in String type, without changing to other data types. In many streaming processing applications, the data sources usually have little to no processing power (e.g. sensors). To simulate this, we shouldn’t do any processing/transformation at the producer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka3 import KafkaProducer\n",
    "import csv\n",
    "import random\n",
    "import datetime as dt\n",
    "\n",
    "hostip = \"kafka\"\n",
    "topic = 'application_stream'\n",
    "\n",
    "def read_csv(filename):\n",
    "    line_list = []\n",
    "    with open(filename, 'r+') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for line in reader:\n",
    "            line_list.append(line)\n",
    "    return line_list\n",
    "\n",
    "def publish_message(producer, topic, data):\n",
    "    try:\n",
    "        producer.send(topic, data)\n",
    "        print('Message published successfully. Data: ')\n",
    "        for d in data:\n",
    "            print(str(d))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "  \n",
    "        \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=[f'{hostip}:9092'],\n",
    "                                  value_serializer=lambda x: dumps(x).encode('ascii'),\n",
    "                                  api_version=(0, 10))\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "        \n",
    "    finally:\n",
    "        return _producer\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    line_list = read_csv('application_data_stream.csv')\n",
    "    print('Publishing records..')\n",
    "    producer = connect_kafka_producer()\n",
    "    \n",
    "    try:\n",
    "        start_index = 0\n",
    "        while True: \n",
    "            num_lines = random.randint(100, 500)\n",
    "            #print(num_lines)\n",
    "            \n",
    "            if num_lines <= len(line_list[start_index:]):\n",
    "                to_send = line_list[start_index : start_index + num_lines]\n",
    "                start_index = start_index + num_lines\n",
    "                \n",
    "            else:\n",
    "                lines_left = num_lines - len(line_list[start_index:])\n",
    "                to_send = line_list[start_index:] + line_list[:lines_left]\n",
    "                start_index = lines_left\n",
    "                \n",
    "            ts = {'ts': int(dt.datetime.now().timestamp())}\n",
    "            data = [dict(record, **ts) for record in to_send]\n",
    "            \n",
    "            publish_message(producer, topic, data)\n",
    "            sleep(5)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print('Stopping the data generation loop.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
